{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7823e7-f5ab-4632-88bc-12f7f8630ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import logging\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class FinancialDataProcessor:\n",
    "    \"\"\"Enhanced financial document processor with advanced cleaning and chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_size: int = 512):\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def clean_financial_text(self, text: str) -> str:\n",
    "        \"\"\"Advanced text cleaning for financial documents.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "            \n",
    "        # Remove HTML tags and entities\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'&[a-zA-Z]+;', '', text)\n",
    "        \n",
    "        # Normalize financial numbers (preserve format)\n",
    "        text = re.sub(r'\\$\\s+', '$', text)  # Fix spaced currency\n",
    "        text = re.sub(r'(\\d)\\s+%', r'\\1%', text)  # Fix spaced percentages\n",
    "        \n",
    "        # Clean whitespace but preserve structure\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Double newlines\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove section headers noise\n",
    "        text = re.sub(r'^[A-Z\\s]{3,}$', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def intelligent_chunking(self, text: str, overlap: int = 50) -> List[Dict]:\n",
    "        \"\"\"Smart chunking that preserves financial context.\"\"\"\n",
    "        cleaned_text = self.clean_financial_text(text)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_length = 0\n",
    "        \n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence_length = len(sentence.split())\n",
    "            \n",
    "            # If adding this sentence exceeds limit, save current chunk\n",
    "            if current_length + sentence_length > self.max_chunk_size and current_chunk:\n",
    "                chunks.append({\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'length': current_length,\n",
    "                    'chunk_id': len(chunks)\n",
    "                })\n",
    "                \n",
    "                # Start new chunk with overlap\n",
    "                overlap_text = self._get_overlap_text(current_chunk, overlap)\n",
    "                current_chunk = overlap_text + \" \" + sentence\n",
    "                current_length = len(current_chunk.split())\n",
    "            else:\n",
    "                current_chunk += \" \" + sentence\n",
    "                current_length += sentence_length\n",
    "        \n",
    "        # Add final chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'length': current_length,\n",
    "                'chunk_id': len(chunks)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _get_overlap_text(self, text: str, overlap_words: int) -> str:\n",
    "        \"\"\"Get last N words for overlap.\"\"\"\n",
    "        words = text.split()\n",
    "        return \" \".join(words[-overlap_words:]) if len(words) > overlap_words else text\n",
    "    \n",
    "    def extract_financial_metrics(self, text: str) -> Dict:\n",
    "        \"\"\"Extract key financial metrics using regex patterns.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Revenue patterns\n",
    "        revenue_patterns = [\n",
    "            r'revenue[s]?\\s*[:\\-]?\\s*\\$?([0-9,.]+)\\s*(million|billion|m|b)?',\n",
    "            r'net\\s+sales[s]?\\s*[:\\-]?\\s*\\$?([0-9,.]+)\\s*(million|billion|m|b)?'\n",
    "        ]\n",
    "        \n",
    "        # Profit patterns\n",
    "        profit_patterns = [\n",
    "            r'net\\s+income[s]?\\s*[:\\-]?\\s*\\$?([0-9,.]+)\\s*(million|billion|m|b)?',\n",
    "            r'profit[s]?\\s*[:\\-]?\\s*\\$?([0-9,.]+)\\s*(million|billion|m|b)?'\n",
    "        ]\n",
    "        \n",
    "        # EPS patterns\n",
    "        eps_patterns = [\n",
    "            r'earnings\\s+per\\s+share[s]?\\s*[:\\-]?\\s*\\$?([0-9,.]+)',\n",
    "            r'eps[s]?\\s*[:\\-]?\\s*\\$?([0-9,.]+)'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for pattern in revenue_patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                metrics['revenue'] = self._normalize_financial_number(match.group(1), match.group(2))\n",
    "                break\n",
    "                \n",
    "        for pattern in profit_patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                metrics['profit'] = self._normalize_financial_number(match.group(1), match.group(2))\n",
    "                break\n",
    "                \n",
    "        for pattern in eps_patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                metrics['eps'] = float(match.group(1).replace(',', ''))\n",
    "                break\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _normalize_financial_number(self, number_str: str, unit: str) -> float:\n",
    "        \"\"\"Convert financial numbers to standard format.\"\"\"\n",
    "        # Clean the number string first\n",
    "        cleaned_number = number_str.replace(',', '').strip()\n",
    "    \n",
    "    # Check if it's actually a valid number\n",
    "        if not cleaned_number or cleaned_number == '.' or not any(c.isdigit() for c in cleaned_number):\n",
    "            return 0.0  # Return 0 for invalid numbers\n",
    "    \n",
    "        try:\n",
    "            number = float(cleaned_number)\n",
    "        \n",
    "            if unit and unit.lower() in ['billion', 'b']:\n",
    "                number *= 1e9\n",
    "            elif unit and unit.lower() in ['million', 'm']:\n",
    "                number *= 1e6\n",
    "            \n",
    "            return number\n",
    "        except ValueError:\n",
    "            return 0.0  # Return 0 if conversion fails\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd8a56e-e471-4c5b-aea8-2a1753aa40d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded data for 40 companies\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# NEW CELL IN data_processor.ipynb\n",
    "def prepare_streamlit_data():\n",
    "    \"\"\"Prepare data specifically for Streamlit interface\"\"\"\n",
    "    \n",
    "    # Load your existing JSONL files\n",
    "    companies_data = {}\n",
    "    \n",
    "    try:\n",
    "        with open('/Users/puchku-home/Downloads/Hackathon Project 2/financebench-main/financebench-main/data/financebench_document_information.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                company = doc.get('company', 'Unknown')\n",
    "                if company not in companies_data:\n",
    "                    companies_data[company] = []\n",
    "                companies_data[company].append(doc)\n",
    "        \n",
    "        print(f\"✅ Loaded data for {len(companies_data)} companies\")\n",
    "        \n",
    "        # Save processed data for quick Streamlit access\n",
    "        with open('streamlit_companies_data.json', 'w') as f:\n",
    "            json.dump(companies_data, f, indent=2)\n",
    "            \n",
    "        return companies_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing data: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "# Run data preparation\n",
    "streamlit_data = prepare_streamlit_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
