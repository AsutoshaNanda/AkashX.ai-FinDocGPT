{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22acb246-7c49-4f13-ae7a-1352defe517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puchku-home/Downloads/Hackathon Project 2/.venv1/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.29.172:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import pandas as pd\n",
    "\n",
    "# Import our modules\n",
    "#from pdf_processor import FinanceBenchProcessor\n",
    "#from data_processor import FinancialDataProcessor\n",
    "#from retrieval_system import AdvancedRetrievalSystem\n",
    "#from qa_system import FinancialQASystem\n",
    "#from sentiment_analyzer import FinancialSentimentAnalyzer\n",
    "#from anomaly_detector import FinancialAnomalyDetector\n",
    "\n",
    "\n",
    "%run pdf_processor.ipynb\n",
    "%run data_processor.ipynb\n",
    "%run retrieval_system.ipynb\n",
    "%run qa_system.ipynb\n",
    "%run sentiment_analyzer.ipynb\n",
    "%run anomaly_detector.ipynb\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize systems\n",
    "fb_processor = None\n",
    "processor = FinancialDataProcessor()\n",
    "retrieval_system = AdvancedRetrievalSystem()\n",
    "qa_system = FinancialQASystem()\n",
    "sentiment_analyzer = FinancialSentimentAnalyzer()\n",
    "anomaly_detector = FinancialAnomalyDetector()\n",
    "\n",
    "# Global variables\n",
    "documents_loaded = False\n",
    "questions_loaded = False\n",
    "all_documents = {}\n",
    "all_questions = []\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'documents_loaded': documents_loaded,\n",
    "        'questions_loaded': questions_loaded\n",
    "    })\n",
    "\n",
    "@app.route('/initialize', methods=['POST'])\n",
    "def initialize_system():\n",
    "    \"\"\"Initialize the system with FinanceBench data.\"\"\"\n",
    "    global fb_processor, documents_loaded, questions_loaded, all_documents, all_questions\n",
    "    \n",
    "    try:\n",
    "        data = request.json\n",
    "        pdfs_dir = data.get('pdfs_dir', 'pdfs')\n",
    "        data_dir = data.get('data_dir', 'data')\n",
    "        \n",
    "        # Initialize processor\n",
    "        fb_processor = FinanceBenchProcessor(pdfs_dir, data_dir)\n",
    "        \n",
    "        # Load all documents\n",
    "        logger.info(\"Loading PDF documents...\")\n",
    "        all_documents = fb_processor.load_all_documents()\n",
    "        \n",
    "        # Load all questions\n",
    "        logger.info(\"Loading questions...\")\n",
    "        all_questions = fb_processor.get_all_questions()\n",
    "        \n",
    "        # Process documents for retrieval\n",
    "        logger.info(\"Processing documents for retrieval...\")\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc_name, doc_data in all_documents.items():\n",
    "            chunks = processor.intelligent_chunking(doc_data['text'])\n",
    "            \n",
    "            # Add metadata to chunks\n",
    "            for chunk in chunks:\n",
    "                chunk['document_id'] = doc_name\n",
    "                chunk['document_metadata'] = {\n",
    "                    'doc_name': doc_name,\n",
    "                    'company': doc_data['company'],\n",
    "                    'doc_type': doc_data['doc_type'],\n",
    "                    'doc_period': doc_data['doc_period'],\n",
    "                    'gics_sector': doc_data['gics_sector']\n",
    "                }\n",
    "            \n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        # Build retrieval index\n",
    "        logger.info(\"Building retrieval index...\")\n",
    "        retrieval_system.build_index(all_chunks)\n",
    "        \n",
    "        documents_loaded = True\n",
    "        questions_loaded = True\n",
    "        \n",
    "        return jsonify({\n",
    "            'message': 'FinanceBench system initialized successfully',\n",
    "            'total_documents': len(all_documents),\n",
    "            'total_questions': len(all_questions),\n",
    "            'total_chunks': len(all_chunks),\n",
    "            'companies': list(set(doc['company'] for doc in all_documents.values()))\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing system: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask_question():\n",
    "    \"\"\"Answer questions using the loaded documents with debug output.\"\"\"\n",
    "    if not documents_loaded:\n",
    "        return jsonify({'error': 'System not initialized. Call /initialize first.'}), 400\n",
    "    \n",
    "    try:\n",
    "        data = request.json\n",
    "        question = data.get('question', '')\n",
    "        company_filter = data.get('company', None)\n",
    "        doc_type_filter = data.get('doc_type', None)\n",
    "        top_k = data.get('top_k', 5)\n",
    "        \n",
    "        if not question:\n",
    "            return jsonify({'error': 'No question provided'}), 400\n",
    "        \n",
    "        print(f\"\\nðŸŒŸ QUESTION: {question}\")\n",
    "        print(f\"ðŸ¢ Company filter: {company_filter}\")\n",
    "        print(f\"ðŸ“„ Doc type filter: {doc_type_filter}\")\n",
    "        \n",
    "        # Retrieve relevant contexts\n",
    "        contexts = retrieval_system.search(question, top_k=top_k)\n",
    "        print(f\"Retrieved {len(contexts)} initial chunks\")\n",
    "        \n",
    "        # Filter contexts if filters provided\n",
    "        if company_filter or doc_type_filter:\n",
    "            filtered_contexts = []\n",
    "            for context in contexts:\n",
    "                metadata = context.get('document_metadata', {})\n",
    "                if company_filter and metadata.get('company') != company_filter:\n",
    "                    continue\n",
    "                if doc_type_filter and metadata.get('doc_type') != doc_type_filter:\n",
    "                    continue\n",
    "                filtered_contexts.append(context)\n",
    "            contexts = filtered_contexts\n",
    "            print(f\"After filtering: {len(contexts)} chunks remain\")\n",
    "        \n",
    "        print(f\"Final retrieved chunks:\")\n",
    "        for idx, chunk in enumerate(contexts):\n",
    "            print(f\"\\n--- Chunk {idx+1} ---\")\n",
    "            # Show chunk text preview\n",
    "            chunk_text = chunk.get('text', chunk.get('content', ''))\n",
    "            print(f\"Text preview: {chunk_text[:500]}\")\n",
    "            # Show metadata if available\n",
    "            metadata = chunk.get('document_metadata', {})\n",
    "            if metadata:\n",
    "                print(f\"Metadata: Company={metadata.get('company')}, Doc={metadata.get('doc_name')}\")\n",
    "        \n",
    "        # Prepare context for QA (handle both formats)\n",
    "        if contexts and isinstance(contexts[0], dict):\n",
    "            if 'text' in contexts[0]:\n",
    "                context_text = \"\\n\\n\".join([chunk['text'] for chunk in contexts])\n",
    "            else:\n",
    "                context_text = \"\\n\\n\".join([chunk.get('content', '') for chunk in contexts])\n",
    "        else:\n",
    "            context_text = \"\"\n",
    "        \n",
    "        print(f\"\\nCombined context length: {len(context_text)} characters\")\n",
    "        print(f\"Context preview (first 300 chars): {context_text[:300]}...\")\n",
    "        \n",
    "        if not context_text.strip():\n",
    "            print(\"âš ï¸ WARNING: Empty context for QA!\")\n",
    "            return jsonify({\n",
    "                'question': question,\n",
    "                'answer': 'No relevant information found',\n",
    "                'confidence': 0.0,\n",
    "                'retrieved_contexts': len(contexts),\n",
    "                'error': 'No relevant context retrieved'\n",
    "            })\n",
    "        \n",
    "        # Generate answer\n",
    "        answer_result = qa_system.answer_question(question, context_text)\n",
    "        print(f\"QA Result: {answer_result}\")\n",
    "        \n",
    "        return jsonify({\n",
    "            'question': question,\n",
    "            'answer': answer_result.get('answer', ''),\n",
    "            'confidence': answer_result.get('confidence', 0),\n",
    "            'qa_confidence': answer_result.get('qa_confidence', 0),\n",
    "            'similarity_confidence': answer_result.get('similarity_confidence', 0),\n",
    "            'combined_confidence': answer_result.get('combined_confidence', 0),\n",
    "            'source_chunks': answer_result.get('source_chunks', []),\n",
    "            'context_preview': context_text[:200] + \"...\" if len(context_text) > 200 else context_text,\n",
    "            'retrieved_contexts': len(contexts),\n",
    "            'filters_applied': {\n",
    "                'company': company_filter,\n",
    "                'doc_type': doc_type_filter\n",
    "            },\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Q&A: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        print(f\"âŒ ERROR in /ask: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "\n",
    "@app.route('/evaluate_benchmarks', methods=['POST'])\n",
    "def evaluate_benchmarks():\n",
    "    \"\"\"Evaluate system on FinanceBench questions.\"\"\"\n",
    "    if not documents_loaded or not questions_loaded:\n",
    "        return jsonify({'error': 'System not initialized. Call /initialize first.'}), 400\n",
    "    \n",
    "    try:\n",
    "        data = request.json\n",
    "        max_questions = data.get('max_questions', 10)  # Limit for demo\n",
    "        company_filter = data.get('company', None)\n",
    "        question_type_filter = data.get('question_type', None)\n",
    "        \n",
    "        # Filter questions\n",
    "        questions_to_evaluate = all_questions.copy()\n",
    "        \n",
    "        if company_filter:\n",
    "            questions_to_evaluate = [q for q in questions_to_evaluate if q['company'] == company_filter]\n",
    "        \n",
    "        if question_type_filter:\n",
    "            questions_to_evaluate = [q for q in questions_to_evaluate if q['question_type'] == question_type_filter]\n",
    "        \n",
    "        # Limit number of questions\n",
    "        questions_to_evaluate = questions_to_evaluate[:max_questions]\n",
    "        \n",
    "        results = []\n",
    "        correct_answers = 0\n",
    "        \n",
    "        for question_data in questions_to_evaluate:\n",
    "            question = question_data['question']\n",
    "            gold_answer = question_data['answer']\n",
    "            \n",
    "            # Get relevant contexts\n",
    "            contexts = retrieval_system.search(question, top_k=5)\n",
    "            \n",
    "            # Filter contexts by document\n",
    "            doc_name = question_data['doc_name']\n",
    "            filtered_contexts = [\n",
    "                ctx for ctx in contexts \n",
    "                if ctx.get('document_metadata', {}).get('doc_name') == doc_name\n",
    "            ]\n",
    "            \n",
    "            # If no contexts from the specific document, use all contexts\n",
    "            if not filtered_contexts:\n",
    "                filtered_contexts = contexts\n",
    "            \n",
    "            # Generate answer\n",
    "            answer_result = qa_system.answer_question(question, filtered_contexts)\n",
    "            \n",
    "            # Simple evaluation (you can make this more sophisticated)\n",
    "            is_correct = _evaluate_answer(answer_result['answer'], gold_answer)\n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            \n",
    "            results.append({\n",
    "                'financebench_id': question_data['financebench_id'],\n",
    "                'question': question,\n",
    "                'predicted_answer': answer_result['answer'],\n",
    "                'gold_answer': gold_answer,\n",
    "                'is_correct': is_correct,\n",
    "                'confidence': answer_result['confidence'],\n",
    "                'company': question_data['company'],\n",
    "                'question_type': question_data['question_type'],\n",
    "                'doc_name': question_data['doc_name']\n",
    "            })\n",
    "        \n",
    "        accuracy = correct_answers / len(results) if results else 0\n",
    "        \n",
    "        return jsonify({\n",
    "            'evaluation_results': results,\n",
    "            'summary': {\n",
    "                'total_questions': len(results),\n",
    "                'correct_answers': correct_answers,\n",
    "                'accuracy': accuracy,\n",
    "                'average_confidence': sum(r['confidence'] for r in results) / len(results) if results else 0\n",
    "            },\n",
    "            'filters_applied': {\n",
    "                'company': company_filter,\n",
    "                'question_type': question_type_filter,\n",
    "                'max_questions': max_questions\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in benchmark evaluation: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "def _evaluate_answer(predicted: str, gold: str) -> bool:\n",
    "    \"\"\"Simple answer evaluation - you can make this more sophisticated.\"\"\"\n",
    "    # Remove common variations and normalize\n",
    "    predicted = predicted.lower().strip()\n",
    "    gold = gold.lower().strip()\n",
    "    \n",
    "    # Check if key numbers match\n",
    "    import re\n",
    "    pred_numbers = re.findall(r'[\\d,]+\\.?\\d*', predicted)\n",
    "    gold_numbers = re.findall(r'[\\d,]+\\.?\\d*', gold)\n",
    "    \n",
    "    if pred_numbers and gold_numbers:\n",
    "        # If both have numbers, check if any match\n",
    "        return any(pnum in gold_numbers for pnum in pred_numbers)\n",
    "    \n",
    "    # Otherwise, check for substring match\n",
    "    return gold in predicted or predicted in gold\n",
    "\n",
    "@app.route('/company_analysis', methods=['POST'])\n",
    "def company_analysis():\n",
    "    \"\"\"Perform comprehensive analysis for a specific company.\"\"\"\n",
    "    if not documents_loaded:\n",
    "        return jsonify({'error': 'System not initialized. Call /initialize first.'}), 400\n",
    "    \n",
    "    try:\n",
    "        data = request.json\n",
    "        company = data.get('company', '')\n",
    "        \n",
    "        if not company:\n",
    "            return jsonify({'error': 'No company specified'}), 400\n",
    "        \n",
    "        # Get company documents\n",
    "        company_docs = {k: v for k, v in all_documents.items() if v['company'] == company}\n",
    "        \n",
    "        if not company_docs:\n",
    "            return jsonify({'error': f'No documents found for company: {company}'}), 400\n",
    "        \n",
    "        # Get company questions\n",
    "        company_questions = [q for q in all_questions if q['company'] == company]\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sentiment_results = []\n",
    "        for doc_name, doc_data in company_docs.items():\n",
    "            chunks = processor.intelligent_chunking(doc_data['text'])\n",
    "            doc_sentiment = sentiment_analyzer.analyze_chunks(chunks)\n",
    "            aggregated_sentiment = sentiment_analyzer.aggregate_sentiment(doc_sentiment)\n",
    "            \n",
    "            sentiment_results.append({\n",
    "                'doc_name': doc_name,\n",
    "                'doc_type': doc_data['doc_type'],\n",
    "                'doc_period': doc_data['doc_period'],\n",
    "                'sentiment': aggregated_sentiment\n",
    "            })\n",
    "        \n",
    "        # Extract financial metrics\n",
    "        financial_metrics = []\n",
    "        for doc_name, doc_data in company_docs.items():\n",
    "            metrics = processor.extract_financial_metrics(doc_data['text'])\n",
    "            if metrics:\n",
    "                metrics['doc_name'] = doc_name\n",
    "                metrics['doc_period'] = doc_data['doc_period']\n",
    "                financial_metrics.append(metrics)\n",
    "        \n",
    "        # Anomaly detection\n",
    "        anomaly_results = None\n",
    "        if financial_metrics:\n",
    "            anomaly_results = anomaly_detector.detect_financial_metric_anomalies(financial_metrics)\n",
    "        \n",
    "        return jsonify({\n",
    "            'company': company,\n",
    "            'documents_analyzed': len(company_docs),\n",
    "            'questions_available': len(company_questions),\n",
    "            'sentiment_analysis': sentiment_results,\n",
    "            'financial_metrics': financial_metrics,\n",
    "            'anomaly_detection': anomaly_results,\n",
    "            'document_types': list(set(doc['doc_type'] for doc in company_docs.values())),\n",
    "            'time_periods': sorted(list(set(doc['doc_period'] for doc in company_docs.values())))\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in company analysis: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/list_companies', methods=['GET'])\n",
    "def list_companies():\n",
    "    \"\"\"List all available companies.\"\"\"\n",
    "    if not documents_loaded:\n",
    "        return jsonify({'error': 'System not initialized. Call /initialize first.'}), 400\n",
    "    \n",
    "    companies = {}\n",
    "    for doc_name, doc_data in all_documents.items():\n",
    "        company = doc_data['company']\n",
    "        if company not in companies:\n",
    "            companies[company] = {\n",
    "                'company': company,\n",
    "                'documents': 0,\n",
    "                'doc_types': set(),\n",
    "                'periods': set(),\n",
    "                'sector': doc_data['gics_sector']\n",
    "            }\n",
    "        \n",
    "        companies[company]['documents'] += 1\n",
    "        companies[company]['doc_types'].add(doc_data['doc_type'])\n",
    "        companies[company]['periods'].add(doc_data['doc_period'])\n",
    "    \n",
    "    # Convert sets to lists for JSON serialization\n",
    "    for company_data in companies.values():\n",
    "        company_data['doc_types'] = list(company_data['doc_types'])\n",
    "        company_data['periods'] = sorted(list(company_data['periods']))\n",
    "    \n",
    "    return jsonify({\n",
    "        'companies': list(companies.values()),\n",
    "        'total_companies': len(companies)\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, use_reloader=False, host='0.0.0.0', port=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb58259-970b-498e-8d20-d0e0a0c276b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
