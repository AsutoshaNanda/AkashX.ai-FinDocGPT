{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc75f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced Retrieval System initialized for Streamlit\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "class AdvancedRetrievalSystem:\n",
    "    \"\"\"High-performance semantic search with FAISS indexing.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def build_index(self, chunks: List[Dict], save_path: str = None):\n",
    "        \"\"\"Build FAISS index from document chunks.\"\"\"\n",
    "        self.chunks = chunks\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        \n",
    "        self.logger.info(f\"Generating embeddings for {len(texts)} chunks...\")\n",
    "        self.embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        normalized_embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "        self.index.add(normalized_embeddings.astype('float32'))\n",
    "        \n",
    "        self.logger.info(f\"Built FAISS index with {self.index.ntotal} vectors\")\n",
    "        \n",
    "        # Save index if path provided\n",
    "        if save_path:\n",
    "            self.save_index(save_path)\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Semantic search with confidence scoring.\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.chunks):  # Valid index\n",
    "                result = self.chunks[idx].copy()\n",
    "                result['similarity_score'] = float(score)\n",
    "                result['confidence'] = self._calculate_confidence(score)\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_confidence(self, similarity_score: float) -> str:\n",
    "        \"\"\"Convert similarity score to confidence level.\"\"\"\n",
    "        if similarity_score > 0.8:\n",
    "            return \"high\"\n",
    "        elif similarity_score > 0.6:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def save_index(self, save_path: str):\n",
    "        \"\"\"Save index and metadata.\"\"\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, os.path.join(save_path, \"faiss_index.bin\"))\n",
    "        \n",
    "        # Save chunks and embeddings\n",
    "        with open(os.path.join(save_path, \"chunks.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.chunks, f)\n",
    "        \n",
    "        with open(os.path.join(save_path, \"embeddings.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.embeddings, f)\n",
    "    \n",
    "    def load_index(self, save_path: str):\n",
    "        \"\"\"Load pre-built index.\"\"\"\n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(os.path.join(save_path, \"faiss_index.bin\"))\n",
    "        \n",
    "        # Load chunks and embeddings\n",
    "        with open(os.path.join(save_path, \"chunks.pkl\"), 'rb') as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "        \n",
    "        with open(os.path.join(save_path, \"embeddings.pkl\"), 'rb') as f:\n",
    "            self.embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "class StreamlitRetrieval:\n",
    "    def __init__(self):\n",
    "        self.retrieval = AdvancedRetrievalSystem()  # Changed from RetrievalSystem to AdvancedRetrievalSystem\n",
    "        \n",
    "    def search_for_streamlit(self, query, company, top_k=5):\n",
    "        try:\n",
    "            results = self.retrieval.search(query, top_k=top_k)\n",
    "            \n",
    "            formatted_results = []\n",
    "            for result in results:\n",
    "                formatted_results.append({\n",
    "                    \"text\": result.get(\"text\", \"\")[:200] + \"...\",\n",
    "                    \"score\": result.get(\"similarity_score\", 0.0),  # Changed from \"score\" to \"similarity_score\"\n",
    "                    \"confidence\": result.get(\"confidence\", \"low\"),\n",
    "                    \"company\": company\n",
    "                })\n",
    "            \n",
    "            return formatted_results\n",
    "        except Exception as e:\n",
    "            return [{\"text\": f\"Error: {str(e)}\", \"score\": 0.0, \"company\": company}]\n",
    "\n",
    "\n",
    "# Initialize the retrieval system\n",
    "streamlit_retrieval = StreamlitRetrieval()\n",
    "print(\"✅ Advanced Retrieval System initialized for Streamlit\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
