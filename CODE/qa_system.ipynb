{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a1d46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Loading FinancialQA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FinancialQA System initialized for Streamlit\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "\n",
    "\n",
    "class FinancialQASystem:\n",
    "    \"\"\"Advanced Q&A system optimized for financial documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"deepset/roberta-base-squad2\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        self.pipeline = pipeline(\"question-answering\", \n",
    "                               model=self.model, \n",
    "                               tokenizer=self.tokenizer,\n",
    "                               device=0 if torch.cuda.is_available() else -1)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def answer_question(self, question: str, context) -> dict:\n",
    "        \"\"\"Answer a question using the provided context.\"\"\"\n",
    "        try:\n",
    "            # Handle both string context and dict context\n",
    "            if isinstance(context, str):\n",
    "                context_text = context\n",
    "            elif isinstance(context, dict) and 'text' in context:\n",
    "                context_text = context['text']\n",
    "            elif isinstance(context, list):\n",
    "                # Handle list of context chunks\n",
    "                context_text = \"\\n\\n\".join([\n",
    "                    chunk['text'] if isinstance(chunk, dict) else str(chunk) \n",
    "                    for chunk in context\n",
    "                ])\n",
    "            else:\n",
    "                context_text = str(context)\n",
    "        \n",
    "            print(f\"QA processing - Question length: {len(question)}, Context length: {len(context_text)}\")\n",
    "        \n",
    "            # Truncate context if too long (BERT models have token limits)\n",
    "            max_context_length = 2000  # Adjust based on your model\n",
    "            if len(context_text.split()) > max_context_length:\n",
    "                context_text = ' '.join(context_text.split()[:max_context_length])\n",
    "                print(f\"Context truncated to {len(context_text.split())} words\")\n",
    "        \n",
    "            # Use the QA pipeline\n",
    "            result = self.pipeline(question=question, context=context_text)\n",
    "        \n",
    "            print(f\"QA pipeline result: {result}\")\n",
    "        \n",
    "            return {\n",
    "                'answer': result.get('answer', ''),\n",
    "                'confidence': result.get('score', 0.0),\n",
    "                'qa_confidence': result.get('score', 0.0),\n",
    "                'similarity_confidence': 0.8,  # Placeholder\n",
    "                'combined_confidence': result.get('score', 0.0),\n",
    "                'source_chunks': [],\n",
    "                'context_preview': context_text[:200] + \"...\" if len(context_text) > 200 else context_text\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"QA Error: {e}\")\n",
    "            return {\n",
    "                'answer': '',\n",
    "                'confidence': 0.0,\n",
    "                'qa_confidence': 0.0,\n",
    "                'similarity_confidence': 0.0,\n",
    "                'combined_confidence': 0.0,\n",
    "                'source_chunks': [],\n",
    "                'context_preview': '',\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def _select_best_answer(self, candidates: List[Dict], question: str) -> Dict:\n",
    "        \"\"\"Select best answer using multiple criteria.\"\"\"\n",
    "        # Score each candidate\n",
    "        for candidate in candidates:\n",
    "            # Combined score: QA confidence + similarity + answer quality\n",
    "            qa_score = candidate['confidence']\n",
    "            similarity_score = candidate['similarity_score']\n",
    "            \n",
    "            # Answer quality heuristics\n",
    "            answer_length_score = min(len(candidate['answer'].split()) / 10, 1.0)  # Prefer moderate length\n",
    "            numeric_bonus = 0.1 if any(char.isdigit() for char in candidate['answer']) else 0  # Financial answers often have numbers\n",
    "            \n",
    "            candidate['combined_score'] = (\n",
    "                qa_score * 0.5 + \n",
    "                similarity_score * 0.3 + \n",
    "                answer_length_score * 0.1 + \n",
    "                numeric_bonus * 0.1\n",
    "            )\n",
    "        \n",
    "        # Return candidate with highest combined score\n",
    "        return max(candidates, key=lambda x: x['combined_score'])\n",
    "    \n",
    "    def _calculate_combined_confidence(self, answer_data: Dict) -> float:\n",
    "        \"\"\"Calculate overall confidence score.\"\"\"\n",
    "        qa_conf = answer_data['confidence']\n",
    "        sim_conf = answer_data['similarity_score']\n",
    "        \n",
    "        # Weighted combination\n",
    "        return (qa_conf * 0.7 + sim_conf * 0.3)\n",
    "    \n",
    "    def batch_qa(self, questions: List[str], retrieval_system, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Process multiple questions efficiently.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for question in questions:\n",
    "            contexts = retrieval_system.search(question, top_k=top_k)\n",
    "            answer = self.answer_question(question, contexts)\n",
    "            answer['question'] = question\n",
    "            results.append(answer)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class StreamlitQAInterface:\n",
    "    def __init__(self):\n",
    "        # Use FinancialQASystem instead of QASystem\n",
    "        self.qa_system = FinancialQASystem()  # Fixed class name\n",
    "        \n",
    "    def process_streamlit_question(self, question, context):  # Changed parameter from 'company' to 'context'\n",
    "        \"\"\"Process question for Streamlit interface\"\"\"\n",
    "        try:\n",
    "            # Use the answer_question method with proper context\n",
    "            result = self.qa_system.answer_question(question, context)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": result.get(\"answer\", \"No answer found\"),\n",
    "                \"confidence\": result.get(\"confidence\", 0.0),\n",
    "                \"source\": result.get(\"context_preview\", \"Unknown\"),\n",
    "                \"status\": \"success\",\n",
    "                \"qa_confidence\": result.get(\"qa_confidence\", 0.0),\n",
    "                \"combined_confidence\": result.get(\"combined_confidence\", 0.0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"source\": \"Error\",\n",
    "                \"status\": \"error\",\n",
    "                \"qa_confidence\": 0.0,\n",
    "                \"combined_confidence\": 0.0\n",
    "            }\n",
    "\n",
    "\n",
    "# Initialize the QA system\n",
    "print(\"ðŸ¤– Loading FinancialQA model...\")\n",
    "streamlit_qa = StreamlitQAInterface()\n",
    "print(\"âœ… FinancialQA System initialized for Streamlit\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
