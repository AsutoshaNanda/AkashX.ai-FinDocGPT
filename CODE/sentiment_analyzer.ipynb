{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6bc9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Loading FinBERT sentiment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Financial Sentiment Analysis initialized for Streamlit\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "class FinancialSentimentAnalyzer:\n",
    "    \"\"\"Advanced financial sentiment analysis with aggregation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"ProsusAI/finbert\"):\n",
    "        self.model_name = model_name\n",
    "        self.pipeline = pipeline(\"sentiment-analysis\", \n",
    "                               model=model_name,\n",
    "                               tokenizer=model_name,\n",
    "                               device=0 if torch.cuda.is_available() else -1)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Sentiment mapping\n",
    "        self.sentiment_mapping = {\n",
    "            'positive': 1,\n",
    "            'negative': -1,\n",
    "            'neutral': 0\n",
    "        }\n",
    "    \n",
    "    def analyze_text(self, text: str) -> Dict:\n",
    "        \"\"\"Analyze sentiment of a single text.\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return {\n",
    "                'sentiment': 'neutral',\n",
    "                'confidence': 0.0,\n",
    "                'score': 0.0\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Truncate text if too long\n",
    "            max_length = 512\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length]\n",
    "            \n",
    "            result = self.pipeline(text)[0]\n",
    "            \n",
    "            sentiment = result['label'].lower()\n",
    "            confidence = result['score']\n",
    "            \n",
    "            # Convert to numerical score\n",
    "            score = self.sentiment_mapping.get(sentiment, 0) * confidence\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': confidence,\n",
    "                'score': score,\n",
    "                'text_length': len(text)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Sentiment analysis error: {e}\")\n",
    "            return {\n",
    "                'sentiment': 'neutral',\n",
    "                'confidence': 0.0,\n",
    "                'score': 0.0\n",
    "            }\n",
    "    \n",
    "    def analyze_chunks(self, chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Analyze sentiment for multiple document chunks.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            sentiment_result = self.analyze_text(chunk['text'])\n",
    "            \n",
    "            result = {\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'sentiment': sentiment_result['sentiment'],\n",
    "                'confidence': sentiment_result['confidence'],\n",
    "                'score': sentiment_result['score'],\n",
    "                'text_preview': chunk['text'][:100] + \"...\" if len(chunk['text']) > 100 else chunk['text']\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def aggregate_sentiment(self, sentiment_results: List[Dict], \n",
    "                          method: str = 'weighted_average') -> Dict:\n",
    "        \"\"\"Aggregate sentiment scores across chunks.\"\"\"\n",
    "        if not sentiment_results:\n",
    "            return {\n",
    "                'overall_sentiment': 'neutral',\n",
    "                'overall_score': 0.0,\n",
    "                'confidence': 0.0,\n",
    "                'sentiment_distribution': {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "            }\n",
    "        \n",
    "        scores = [r['score'] for r in sentiment_results]\n",
    "        confidences = [r['confidence'] for r in sentiment_results]\n",
    "        sentiments = [r['sentiment'] for r in sentiment_results]\n",
    "        \n",
    "        # Calculate overall score\n",
    "        if method == 'weighted_average':\n",
    "            overall_score = np.average(scores, weights=confidences)\n",
    "        else:  # simple average\n",
    "            overall_score = np.mean(scores)\n",
    "        \n",
    "        # Determine overall sentiment\n",
    "        if overall_score > 0.1:\n",
    "            overall_sentiment = 'positive'\n",
    "        elif overall_score < -0.1:\n",
    "            overall_sentiment = 'negative'\n",
    "        else:\n",
    "            overall_sentiment = 'neutral'\n",
    "        \n",
    "        # Sentiment distribution\n",
    "        sentiment_counts = pd.Series(sentiments).value_counts().to_dict()\n",
    "        total_chunks = len(sentiment_results)\n",
    "        \n",
    "        sentiment_distribution = {\n",
    "            'positive': sentiment_counts.get('positive', 0) / total_chunks,\n",
    "            'negative': sentiment_counts.get('negative', 0) / total_chunks,\n",
    "            'neutral': sentiment_counts.get('neutral', 0) / total_chunks\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'overall_sentiment': overall_sentiment,\n",
    "            'overall_score': overall_score,\n",
    "            'confidence': np.mean(confidences),\n",
    "            'sentiment_distribution': sentiment_distribution,\n",
    "            'total_chunks': total_chunks,\n",
    "            'score_std': np.std(scores)\n",
    "        }\n",
    "    \n",
    "    def temporal_sentiment_analysis(self, documents: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Analyze sentiment trends over time.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # Assume documents have 'date', 'ticker', and 'text' fields\n",
    "            chunks = doc.get('chunks', [{'text': doc.get('text', ''), 'chunk_id': 0}])\n",
    "            sentiment_results = self.analyze_chunks(chunks)\n",
    "            aggregated = self.aggregate_sentiment(sentiment_results)\n",
    "            \n",
    "            results.append({\n",
    "                'date': doc.get('date'),\n",
    "                'ticker': doc.get('ticker'),\n",
    "                'sentiment': aggregated['overall_sentiment'],\n",
    "                'score': aggregated['overall_score'],\n",
    "                'confidence': aggregated['confidence'],\n",
    "                'positive_ratio': aggregated['sentiment_distribution']['positive'],\n",
    "                'negative_ratio': aggregated['sentiment_distribution']['negative'],\n",
    "                'neutral_ratio': aggregated['sentiment_distribution']['neutral']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "class StreamlitSentiment:\n",
    "    def __init__(self):\n",
    "        self.sentiment_analyzer = FinancialSentimentAnalyzer()  # Fixed class name\n",
    "        \n",
    "    def analyze_for_streamlit(self, text):\n",
    "        try:\n",
    "            result = self.sentiment_analyzer.analyze_text(text)\n",
    "            \n",
    "            # Map FinancialSentimentAnalyzer output to expected Streamlit format\n",
    "            sentiment = result.get(\"sentiment\", \"neutral\")\n",
    "            score = result.get(\"score\", 0.0)\n",
    "            confidence = result.get(\"confidence\", 0.0)\n",
    "            \n",
    "            # Convert to compound-style scoring for compatibility\n",
    "            return {\n",
    "                \"sentiment\": sentiment.capitalize(),\n",
    "                \"compound\": score,  # Overall sentiment score\n",
    "                \"confidence\": confidence,\n",
    "                \"score\": score,\n",
    "                \"positive\": confidence if sentiment == 'positive' else 0.0,\n",
    "                \"negative\": confidence if sentiment == 'negative' else 0.0,\n",
    "                \"neutral\": confidence if sentiment == 'neutral' else 0.0,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"sentiment\": \"Error\",\n",
    "                \"compound\": 0.0,\n",
    "                \"confidence\": 0.0,\n",
    "                \"score\": 0.0,\n",
    "                \"positive\": 0.0,\n",
    "                \"negative\": 0.0,\n",
    "                \"neutral\": 1.0,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "print(\"ðŸ¤– Loading FinBERT sentiment model...\")\n",
    "streamlit_sentiment = StreamlitSentiment()\n",
    "print(\"âœ… Financial Sentiment Analysis initialized for Streamlit\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
